{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALI的Tensorflow炼成与GAN科普\n",
    "\n",
    "\n",
    "**Abstract：**Deep Learning是一个很大的领域，其中GAN是Deep Learning的明星，希望大家可以通过本文来简单的了解一下GAN这个模型以及这个模型的一些运用。本文介绍关于GAN的一些知识以及GAN的思想如何转移到ALI中，以及通过Google的Deep Learning框架`TensorFlow`，通过MNIST数据来实现ALI模型。所涉及到Tensorflow的一些很简单的一些解说，包括`tf.Variable()`和`tf.placeholder()`的一些用法以及区别，同时给出一小段代码案例。虽然提到了最简单的两个Tensorflow，`tf.Variable()`和`tf.placeholder()`，但是在本文中，不深入解释更多的关于TensorFlow的运用以及神经网络是如何搭建的。最后，我们将结果组合成模型。\n",
    "\n",
    "## 使用Tensorflow建立ALI模型\n",
    "\n",
    "### 背景\n",
    "\n",
    "由于这次做项目的时候用到ALI这个Model，朋友希望自己能够为ALI和GAN来写一点自己的想法，故作此文。\n",
    "\n",
    "生成模型已经成为建复杂高维数据集模型的常用的框架，面对复杂性时如同降维二向箔打击。\n",
    "\n",
    "---------------------\n",
    "\n",
    "### 什么是ALI模型？\n",
    "\n",
    "ALI(adversarially learned inference)，中文直译过来呢，就是对抗学习推理了吧？\n",
    "\n",
    "在这里简单的给大家介绍一下ALI模型。\n",
    "\n",
    "既然有一个Adversarial在里面，那想必就和GAN有一点关系了。而GAN(Generative Adversarial Networks)即“生成对抗网络”就是Deep Learning入门神书的作者Ian Goodfellow的神作。\n",
    "\n",
    "GAN的idea就是同时训练两个模型，一个生成模型，一个Discriminative Model。我们把生成模型简写成G，Discriminative Model简称为D。而实现的方法，是让两个网络相互竞争。\n",
    "\n",
    "生成模型是用来获取数据的分布情况的，而D则是用来估计来自训练数据的概率的。\n",
    "\n",
    "生成模型的训练过程就是想办法把D的错误概率给最大化。\n",
    "\n",
    "在任意函数G和D的空间中，存在唯一的解决方案，G恢复训练数据分布，D等于$1/2$。 \n",
    "\n",
    "在G和D由多层感知器定义的情况下，整个系统可以用BP来进行训练。 \n",
    "\n",
    "在Training或Generating样本的期间，**不需要任何马尔科夫链或展开的近似推理网络。**，也就是活生生的把推理给绕过去了。当然还有Autoregressive Approaches（自回归方法）放弃潜在的表征，而是直接对输入变量之间的关系进行建模。或多或少的砍掉或者放弃掉一些东西。\n",
    "\n",
    "自回归模型可以生成相当出色的样本，但是牺牲掉了速度。同时其要求学习之前数据的抽象表达。而基于GAN的方法代表了一个很好的妥协：他们学习一个生成模型，生成比最佳VAE技术更高质量的样本，而不牺牲采样速度，并且还利用潜在代表在生成过程中。然而，GAN缺乏有效的推理机制，使得GANs无法在抽象层面推理数据。因此大牛们在研究如何优雅的将GANs其他的方法进行结合，出杂交种，其实本质意义上就是在两个短板中互相妥协罢了。\n",
    "\n",
    "ALI是个通过Generation Network和Inference Network。 两个Model来对怼。也就是将Inference Machine（或encoder）和深度定向G Model（decoder）投入到类似GAN的对抗框架中学习。\n",
    "\n",
    "训练一个鉴别器，以便将来自解码器的联合样本的数据和相应的潜在变量的联合样本与编码器（或近似后验）区分开，而编码器和解码器被一起训练以愚弄鉴别器。我们不仅要求鉴别器区分合成样本与实际数据，而且要求它区分数据空间和潜在变量之间的两个联合分布。生成网络将样本从随机潜在变量映射到数据空间，而推理网络将数据空间中的训练示例映射到潜在变量的空间。\n",
    "\n",
    "我们可以针对GAN得出以下构想：\n",
    "\n",
    "两个一个生成器，一个判别器共两个神经网络重复博弈。\n",
    "\n",
    "![](ALI-概念图.png)\n",
    "\n",
    "--------------------------\n",
    "\n",
    "今天用Tensorflow实现一下，同时解释一些Tensorflow的Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们设置好我们的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mb_size = 32\n",
    "X_dim = 784\n",
    "z_dim = 64\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "d_steps = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入大名鼎鼎的MNIST数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个简单的科普\n",
    "\n",
    "很多人其实经常就是Tensorflow的第一步就愣住了，卧槽？？？这是什么玩意？？？\n",
    "\n",
    "为了让大家不至于——“卧槽这是什么玩意？”\n",
    "\n",
    "简单的给大家说一下`tf.placeholder`：\n",
    "\n",
    "其实大家看模型的时候（包括看Keras或者TensorLayer），其实经常可以看到两个很经典东西：\n",
    "\n",
    "```\n",
    "tf.Variable()\n",
    "\n",
    "tf.placeholder()\n",
    "```\n",
    "\n",
    "这两个有什么区别呢？\n",
    "\n",
    "为了训练我们的example，我们首先要立个placeholder。`tf.placeholder`是用来feed我们需要训练的example的，同时`tf.placeholder`是**必须用**`feed_dict`**来fed！！！**\n",
    "\n",
    "给大家举个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_example= tf.placeholder(tf.float32, shape=(1024, 1024))\n",
    "y_example = tf.matmul(x_example, x_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 262.44055176  264.40164185  261.80392456 ...,  264.89172363\n",
      "   265.04611206  262.62728882]\n",
      " [ 262.56356812  261.39038086  256.91809082 ...,  265.66696167\n",
      "   269.59469604  269.17672729]\n",
      " [ 257.39651489  258.3777771   257.86535645 ...,  261.78411865\n",
      "   254.41978455  256.24316406]\n",
      " ..., \n",
      " [ 256.59402466  260.95648193  259.51330566 ...,  262.58615112\n",
      "   265.44903564  265.43508911]\n",
      " [ 250.66888428  251.14041138  255.3341217  ...,  255.5091095   256.3104248\n",
      "   258.73748779]\n",
      " [ 253.89770508  251.85395813  257.89154053 ...,  251.33482361\n",
      "   254.21363831  258.79104614]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#     print(sess.run(y_example))\n",
    "    rand_array = np.random.rand(1024, 1024)\n",
    "    print(sess.run(y_example, feed_dict={x_example: rand_array}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里呢，`sess.run(y_example)`是会被弄死的，因为你只能用feed_dict来fed我们的`tf.placeholder`\n",
    "\n",
    "但是我们对比一下`tf.Variable`的话，`tf.Variable`是通过`run()`调用来维护状态的，也就是说，在之前的我们的`sess.run(y_example)`是可以受用于`tf.Variable`的。\n",
    "\n",
    "通过构造`tf.Variable`来向图中添加变量。\n",
    "\n",
    "同时`tf.Variable()`的构造函数需要给定任何类型和形状的Tensor变量的初始值。\n",
    "\n",
    "初始值定义变量的类型和形状，其变量的`type`和`shape`是固定的。\n",
    "\n",
    "因此，在这里我们主要使用`tf.placeholder`来feed我们需要训练的example的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK，现在进入正题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, X_dim])\n",
    "z = tf.placeholder(tf.float32, shape=[None, z_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先定义几个简单的小函数，包括一些必要的log和sample的函数以及后期用到的导出结果的可视化函数。\n",
    "\n",
    "由于这一环节没有任何的难点，不做多余阐述："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "def log(x):\n",
    "    return tf.log(x + 1e-8)\n",
    "\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_W1 = tf.Variable(xavier_init([X_dim + z_dim, h_dim]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "D_W2 = tf.Variable(xavier_init([h_dim, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q_W1 = tf.Variable(xavier_init([X_dim, h_dim]))\n",
    "Q_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "Q_W2 = tf.Variable(xavier_init([h_dim, z_dim]))\n",
    "Q_b2 = tf.Variable(tf.zeros(shape=[z_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P_W1 = tf.Variable(xavier_init([z_dim, h_dim]))\n",
    "P_b1 = tf.Variable(tf.zeros(shape=[h_dim]))\n",
    "P_W2 = tf.Variable(xavier_init([h_dim, X_dim]))\n",
    "P_b2 = tf.Variable(tf.zeros(shape=[X_dim]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_G = [Q_W1, Q_W2, Q_b1, Q_b2, P_W1, P_W2, P_b1, P_b2]\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑x和z这两个概率分布：\n",
    "\n",
    "* encoder（编码器）联合分布：$q(x,z)=q(x)q(z|x)$\n",
    "* decoder（解码器）联合分布：$p(x,z)=p(z)p(x|z)$\n",
    "\n",
    "这两种分布的边际我们是已知的，编码器边际$q(x)$是经验数据分布，解码器边缘$p(z)$通常被定义为简单的因式分布。比如标准正态分布$p(z)=N(0,I)$。p和q之间的生成过程相反。\n",
    "\n",
    "ALI的目标是匹配两个联合分布。首先我们要确保所有边界匹配，所有条件分布也可以匹配成功。也就是我们要注意条件$q(z|x)$与$p(z|x)$的匹配。\n",
    "\n",
    "$$\\min\\limits_{G}\\max\\limits_{D}V(D,G)=\\mathbb{E}_{q(x)}[\\log(D(x,G_z(x)))]+\\mathbb{E}_{p(z)}[\\log(1-D(G_x(z),z))]=\\\\ \\int\\int q(x)q(z|x)\\log(D(x,z))dxdz\\\\+\\int\\int p(z)p(x|z)\\log(1-D(x,z))dxdz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在来定义$q(x)$和$p(z)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Q(X):\n",
    "    h = tf.nn.relu(tf.matmul(X, Q_W1) + Q_b1)\n",
    "    h = tf.matmul(h, Q_W2) + Q_b2\n",
    "    return h\n",
    "\n",
    "def P(z):\n",
    "    h = tf.nn.relu(tf.matmul(z, P_W1) + P_b1)\n",
    "    h = tf.matmul(h, P_W2) + P_b2\n",
    "    return tf.nn.sigmoid(h)\n",
    "\n",
    "def D(X, z):\n",
    "    inputs = tf.concat([X, z], axis=1)\n",
    "    h = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    return tf.nn.sigmoid(tf.matmul(h, D_W2) + D_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_hat = Q(X)\n",
    "X_hat = P(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_enc = D(X, z_hat)\n",
    "D_gen = D(X_hat, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_loss = -tf.reduce_mean(log(D_enc) + log(1 - D_gen))\n",
    "G_loss = -tf.reduce_mean(log(D_gen) + log(1 - D_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(D_loss, var_list=theta_D))\n",
    "G_solver = (tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            .minimize(G_loss, var_list=theta_G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们通过`tf.Session()`来跑模型。\n",
    "\n",
    "最后，说一下这个很经常出现`Session()`。\n",
    "\n",
    "Tensorflow以Graph的计算而得名，而`Session()`封装了执行Operation对象的环境。通过Session可以执行图的计算。\n",
    "\n",
    "与`Session`相近的有`tf.InteractiveSession()`\n",
    "\n",
    "`tf.InteractiveSession()`将自己安装为构建中的默认Session。\n",
    "\n",
    "而简单直接的方法也有这般：\n",
    "\n",
    "```python\n",
    "sess = tf.InteractiveSession()\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "# We can just use 'c.eval()' without passing 'sess'\n",
    "print(c.eval())\n",
    "sess.close()\n",
    "```\n",
    "\n",
    "常规会话会在with语句中创建时将其自身作为默认会话。一般我们在代码中的用法是这样的，我也比较常用`Session()`这个方法就是了：\n",
    "\n",
    "```python\n",
    "a = tf.constant(5.0)\n",
    "b = tf.constant(6.0)\n",
    "c = a * b\n",
    "with tf.Session():\n",
    "    # We can also use 'c.eval()' here.\n",
    "    print(c.eval())\n",
    "```\n",
    "\n",
    "总结一下一般的启动逻辑：\n",
    "\n",
    "1. 先定义好变量(Variable或者placeholder)\n",
    "* sess=tf.Session()做一个准备\n",
    "* sess.run()，在run里面进行各种花式的张量计算，这也就是体现技术的point\n",
    "* sess.close()\n",
    "\n",
    "因此，`tensorflow`就将底层给配置好之后，我们在`tensorflow`这个大环境里愉快的奔跑。\n",
    "\n",
    "而难点就在于Variable的各种的设置和run的操作方法。\n",
    "\n",
    "初期，Variable被大量的套路给设定好，我们可以通过别人的轮子，来配适。\n",
    "\n",
    "但是Deep Learning的从业者的核心价值应该在于**能够自己的将自己想做的数据导入自己的模型**，而不是简单的调参或者利用别人的轮子滚来滚去。各种算法和一些主流的实现就像加减乘除一般了然于胸，并且有自己做轮子的能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将模型训练十万次（如果有想法的，请换一台支持GPU的电脑，要不然可能会等很久。），我们将1000Iteration为一个单位，print一次："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; D_loss: 0.003775; G_loss: 26.54\n",
      "Iter: 1000; D_loss: 0.06725; G_loss: 27.79\n",
      "Iter: 2000; D_loss: 0.1544; G_loss: 18.63\n",
      "Iter: 3000; D_loss: 0.09687; G_loss: 17.99\n",
      "Iter: 4000; D_loss: 0.559; G_loss: 14.4\n",
      "Iter: 5000; D_loss: 0.2624; G_loss: 13.34\n",
      "Iter: 6000; D_loss: 0.4579; G_loss: 13.86\n",
      "Iter: 7000; D_loss: 0.5003; G_loss: 14.18\n",
      "Iter: 8000; D_loss: 0.4746; G_loss: 10.15\n",
      "Iter: 9000; D_loss: 0.7304; G_loss: 10.96\n",
      "Iter: 10000; D_loss: 0.6929; G_loss: 8.373\n",
      "Iter: 11000; D_loss: 0.4615; G_loss: 9.82\n",
      "Iter: 12000; D_loss: 0.5451; G_loss: 7.258\n",
      "Iter: 13000; D_loss: 0.4942; G_loss: 7.463\n",
      "Iter: 14000; D_loss: 0.7117; G_loss: 8.421\n",
      "Iter: 15000; D_loss: 0.3711; G_loss: 7.135\n",
      "Iter: 16000; D_loss: 1.271; G_loss: 9.167\n",
      "Iter: 17000; D_loss: 0.379; G_loss: 8.942\n",
      "Iter: 18000; D_loss: 0.444; G_loss: 8.204\n",
      "Iter: 19000; D_loss: 0.573; G_loss: 7.7\n",
      "Iter: 20000; D_loss: 0.4686; G_loss: 7.426\n",
      "Iter: 21000; D_loss: 0.3614; G_loss: 8.388\n",
      "Iter: 22000; D_loss: 0.5678; G_loss: 7.96\n",
      "Iter: 23000; D_loss: 0.38; G_loss: 7.005\n",
      "Iter: 24000; D_loss: 0.2991; G_loss: 8.931\n",
      "Iter: 25000; D_loss: 0.7902; G_loss: 8.061\n",
      "Iter: 26000; D_loss: 0.5132; G_loss: 7.275\n",
      "Iter: 27000; D_loss: 0.8313; G_loss: 6.133\n",
      "Iter: 28000; D_loss: 0.9036; G_loss: 6.114\n",
      "Iter: 29000; D_loss: 0.6095; G_loss: 6.638\n",
      "Iter: 30000; D_loss: 0.7496; G_loss: 6.498\n",
      "Iter: 31000; D_loss: 0.767; G_loss: 5.01\n",
      "Iter: 32000; D_loss: 0.6455; G_loss: 7.366\n",
      "Iter: 33000; D_loss: 0.7684; G_loss: 6.049\n",
      "Iter: 34000; D_loss: 0.7842; G_loss: 5.829\n",
      "Iter: 35000; D_loss: 0.6602; G_loss: 6.678\n",
      "Iter: 36000; D_loss: 0.516; G_loss: 5.699\n",
      "Iter: 37000; D_loss: 0.8214; G_loss: 5.519\n",
      "Iter: 38000; D_loss: 0.8349; G_loss: 5.142\n",
      "Iter: 39000; D_loss: 0.4174; G_loss: 5.673\n",
      "Iter: 40000; D_loss: 0.6856; G_loss: 5.004\n",
      "Iter: 41000; D_loss: 0.9552; G_loss: 4.35\n",
      "Iter: 42000; D_loss: 0.7441; G_loss: 4.228\n",
      "Iter: 43000; D_loss: 1.008; G_loss: 3.79\n",
      "Iter: 44000; D_loss: 0.5039; G_loss: 5.857\n",
      "Iter: 45000; D_loss: 0.8019; G_loss: 4.825\n",
      "Iter: 46000; D_loss: 0.6394; G_loss: 5.24\n",
      "Iter: 47000; D_loss: 1.158; G_loss: 4.659\n",
      "Iter: 48000; D_loss: 0.7164; G_loss: 4.782\n",
      "Iter: 49000; D_loss: 0.7212; G_loss: 5.886\n",
      "Iter: 50000; D_loss: 0.763; G_loss: 5.352\n",
      "Iter: 51000; D_loss: 0.8736; G_loss: 4.57\n",
      "Iter: 52000; D_loss: 0.7512; G_loss: 4.49\n",
      "Iter: 53000; D_loss: 0.7568; G_loss: 4.986\n",
      "Iter: 54000; D_loss: 1.035; G_loss: 4.781\n",
      "Iter: 55000; D_loss: 0.6497; G_loss: 5.281\n",
      "Iter: 56000; D_loss: 0.6902; G_loss: 4.616\n",
      "Iter: 57000; D_loss: 1.327; G_loss: 5.31\n",
      "Iter: 58000; D_loss: 0.7152; G_loss: 4.9\n",
      "Iter: 59000; D_loss: 0.8573; G_loss: 5.34\n",
      "Iter: 60000; D_loss: 0.9777; G_loss: 4.359\n",
      "Iter: 61000; D_loss: 0.7491; G_loss: 4.924\n",
      "Iter: 62000; D_loss: 0.7962; G_loss: 4.51\n",
      "Iter: 63000; D_loss: 0.5643; G_loss: 4.811\n",
      "Iter: 64000; D_loss: 0.6366; G_loss: 5.549\n",
      "Iter: 65000; D_loss: 0.7347; G_loss: 4.387\n",
      "Iter: 66000; D_loss: 0.9506; G_loss: 4.561\n",
      "Iter: 67000; D_loss: 0.9165; G_loss: 4.067\n",
      "Iter: 68000; D_loss: 0.9081; G_loss: 3.806\n",
      "Iter: 69000; D_loss: 0.7897; G_loss: 5.045\n",
      "Iter: 70000; D_loss: 0.6791; G_loss: 4.992\n",
      "Iter: 71000; D_loss: 0.5531; G_loss: 5.66\n",
      "Iter: 72000; D_loss: 0.7581; G_loss: 4.716\n",
      "Iter: 73000; D_loss: 1.007; G_loss: 3.397\n",
      "Iter: 74000; D_loss: 1.098; G_loss: 4.519\n",
      "Iter: 75000; D_loss: 0.7628; G_loss: 4.434\n",
      "Iter: 76000; D_loss: 0.8365; G_loss: 4.765\n",
      "Iter: 77000; D_loss: 0.8605; G_loss: 4.115\n",
      "Iter: 78000; D_loss: 0.9521; G_loss: 4.161\n",
      "Iter: 79000; D_loss: 1.061; G_loss: 4.592\n",
      "Iter: 80000; D_loss: 0.8157; G_loss: 4.658\n",
      "Iter: 81000; D_loss: 0.8301; G_loss: 4.887\n",
      "Iter: 82000; D_loss: 0.6612; G_loss: 4.689\n",
      "Iter: 83000; D_loss: 0.9026; G_loss: 3.83\n",
      "Iter: 84000; D_loss: 0.5562; G_loss: 4.997\n",
      "Iter: 85000; D_loss: 0.5194; G_loss: 4.886\n",
      "Iter: 86000; D_loss: 0.6996; G_loss: 3.93\n",
      "Iter: 87000; D_loss: 0.9773; G_loss: 5.031\n",
      "Iter: 88000; D_loss: 0.7209; G_loss: 5.389\n",
      "Iter: 89000; D_loss: 0.699; G_loss: 5.431\n",
      "Iter: 90000; D_loss: 0.7684; G_loss: 4.338\n",
      "Iter: 91000; D_loss: 0.7682; G_loss: 5.868\n",
      "Iter: 92000; D_loss: 0.5466; G_loss: 4.99\n",
      "Iter: 93000; D_loss: 0.9008; G_loss: 4.793\n",
      "Iter: 94000; D_loss: 0.8325; G_loss: 5.759\n",
      "Iter: 95000; D_loss: 0.6954; G_loss: 4.999\n",
      "Iter: 96000; D_loss: 0.5241; G_loss: 6.134\n",
      "Iter: 97000; D_loss: 0.8004; G_loss: 5.064\n",
      "Iter: 98000; D_loss: 0.9975; G_loss: 5.467\n",
      "Iter: 99000; D_loss: 0.5801; G_loss: 6.017\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for it in range(100000):\n",
    "    X_mb, _ = mnist.train.next_batch(mb_size)\n",
    "    z_mb = sample_z(mb_size, z_dim)\n",
    "\n",
    "    _, D_loss_curr = sess.run(\n",
    "        [D_solver, D_loss], feed_dict={X: X_mb, z: z_mb}\n",
    "    )\n",
    "\n",
    "    _, G_loss_curr = sess.run(\n",
    "        [G_solver, G_loss], feed_dict={X: X_mb, z: z_mb}\n",
    "    )\n",
    "\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter: {}; D_loss: {:.4}; G_loss: {:.4}'\n",
    "              .format(it, D_loss_curr, G_loss_curr))\n",
    "\n",
    "        samples = sess.run(X_hat, feed_dict={z: sample_z(16, z_dim)})\n",
    "\n",
    "        fig = plot(samples)\n",
    "        plt.savefig('out/{}.png'\n",
    "                    .format(str(i).zfill(3)), bbox_inches='tight')\n",
    "        i += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于导出大量的图片，我在这里就给大家看一下对比图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最早的图：\n",
    "\n",
    "![](out\\000.png)\n",
    "\n",
    "不久之后：\n",
    "![](out\\051.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "本文由[ALI Paper, arxiv](https://arxiv.org/abs/1606.00704)依照Tensorflow重现。ALI的概念非本人提出，本人根据自己对ALI的理解通过Tensorflow进行实现，并且运行于Jupyter中。所有代码在Windows10,Python3.5,TensorflowGPU版(1.1.0rc2)完美运行。限于本人水平，可能有出现一定的错误，如有失误，欢迎交流。同时，由于本人学习Deep Learning的时候直接接触英文材料，故一些专有名词的翻译可能存在一些偏差。故一些专有名词直接保留为原单词，不做翻译。ALI的翻译没有参考过任何相关中文信息，由本人直接查阅在Arxiv上ALI的原文所得。\n",
    "\n",
    "如果有想来交流的小伙伴，欢迎私信。\n",
    "\n",
    "-----------------\n",
    "## Reference\n",
    "\n",
    "1. [ALI Paper, arxiv](https://arxiv.org/abs/1606.00704)\n",
    "* [GAN - Paper, NIPS](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)\n",
    "* [GAN - Goodfellow](https://github.com/goodfeli/adversarial)\n",
    "* [生成模型与判别模型 zouxy09,csdn](http://blog.csdn.net/zouxy09/article/details/8195017)\n",
    "* [Adversarial machine learning - ACM Digital Library](https://dl.acm.org/citation.cfm?id=2046692)\n",
    "* [Running Graph](https://www.tensorflow.org/api_guides/python/client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
